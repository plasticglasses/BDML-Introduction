{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ordered-recorder",
   "metadata": {},
   "source": [
    "# CW1 - Object Recognition using CNN\n",
    "To apply machine learning alorithms to clasify the testing images into object categories. Then use a model to perform classification and report quantitative results.\n",
    "\n",
    "Due: Monday 19th April"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-karma",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-turning",
   "metadata": {},
   "source": [
    "The aim is to evaluate the use of CNN's in image recognition and the affect of adding multiple layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-negotiation",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "modified-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import load\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import skimage.feature\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-democracy",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "There are 100 different categories of objects\n",
    "each has 500 images for training and 100 images for testing.\n",
    "Split the data into train and test sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "recovered-longer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images Shape: (32, 32, 3, 50000)\n",
      "Train Labels Fine Shape: (50000,)\n",
      "Train Labels Coarse Shape: (50000,)\n"
     ]
    }
   ],
   "source": [
    "images = np.load('trnImage.npy')\n",
    "label_fine = np.load('trnLabel_fine.npy')\n",
    "label_coarse = np.load('trnLabel_coarse.npy')\n",
    "\n",
    "#image_index = 1 # pick a specific image\n",
    "#image = images[:, :, :, image_index]\n",
    "\n",
    "test_images = np.load('tstImage.npy')\n",
    "test_label_fine = np.load('tstLabel_fine.npy')\n",
    "test_label_coarse = np.load('tstLabel_coarse.npy')\n",
    "\n",
    "print(f'Images Shape: {images.shape}')\n",
    "print(f'Train Labels Fine Shape: {label_fine.shape}')\n",
    "print(f'Train Labels Coarse Shape: {label_coarse.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-wright",
   "metadata": {},
   "source": [
    "# Shuffle data to ensure not ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "martial-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = shuffle(images, random_state=0)\n",
    "label_fine, label_coarse = shuffle(label_fine, label_coarse, random_state=0) #make sure the samples are not ordered\n",
    "\n",
    "\n",
    "test_images = shuffle(test_images, random_state=0)\n",
    "test_label_fine, test_label_coarse = shuffle(test_label_fine, test_label_coarse, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-chain",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#images_reshaped = np.transpose(images, (3, 0, 1, 2))[:, :, :, -1]\n",
    "#images_reshaped = np.expand_dims(images_reshaped, axis=3)\n",
    "#\n",
    "#print(images_reshaped.shape)\n",
    "#print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_images_reshaped = np.transpose(test_images, (3, 0, 1, 2))[:, :, :, -1]\n",
    "#test_images_reshaped = np.expand_dims(test_images_reshaped, axis=3)\n",
    "\n",
    "#print(test_images_reshaped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-constraint",
   "metadata": {},
   "source": [
    "# Normalise the data, for each image do a hog, add how to array, train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "banner-psychiatry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_images_normalised = load('hog_array_train.npy')\n",
    "    print(train_images_normalised.shape)\n",
    "except FileNotFoundError: \n",
    "    \n",
    "    train_images_normalised = []\n",
    "    \n",
    "    for image_index in range(0, images.shape[3]):\n",
    "        print(image_index, images.shape[3])\n",
    "        image = images[:, :, :, image_index]\n",
    "\n",
    "        # Extract features from a single image\n",
    "        _, hog_image = skimage.feature.hog(image, pixels_per_cell=[2,2], cells_per_block=[3,3], visualize=True)\n",
    "        train_images_normalised.append(hog_image)\n",
    "        \n",
    "    train_images_output = np.array(train_images_normalised)\n",
    "    data = asarray(train_images_output)\n",
    "        # save to npy file\n",
    "    save('hog_array_train.npy', data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-tokyo",
   "metadata": {},
   "source": [
    "normalise test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "absolute-arrival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test_images_normalised = load('hog_array_test.npy')\n",
    "    print(test_images_normalised.shape)\n",
    "    \n",
    "except: \n",
    "    test_images_normalised = []\n",
    "    for image_index in range(0, test_images.shape[3]):\n",
    "        print(image_index, test_images.shape[3])\n",
    "        image = test_images[:, :, :, image_index]\n",
    "\n",
    "        # Extract features from a single image\n",
    "        _, hog_image = skimage.feature.hog(image, pixels_per_cell=[2,2], cells_per_block=[3,3], visualize=True)\n",
    "        test_images_normalised.append(hog_image)\n",
    "\n",
    "    train_images_output = np.array(test_images_normalised)\n",
    "    data = asarray(train_images_output)\n",
    "    save('hog_array_test.npy', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-opportunity",
   "metadata": {},
   "source": [
    "### Create and train a Tensorflow Convolutional Neural Network on the training set using Conv2D and pooling Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "interior-penetration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "=================================================================\n",
      "Total params: 320\n",
      "Trainable params: 320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "vietnamese-documentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32)\n",
      "(10000, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(train_images_normalised.shape)\n",
    "print(test_images_normalised.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-guyana",
   "metadata": {},
   "source": [
    "Check the Data cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_images_normalised.shape)\n",
    "# print(test_images_normalised.shape)\n",
    "\n",
    "# #train_images_normalised = np.array(train_images_normalised).reshape(50000, -1, 32, 32)\n",
    "# #test_images_normalised = np.array(test_images_normalised).reshape(10000, -1, 32, 32)\n",
    "\n",
    "# #train_images_normalised = train_images_normalised[:, 0, :, :]\n",
    "# test_images_normalised = test_images_normalised[:, 0, :, :]\n",
    "\n",
    "# train_images_normalised = np.transpose(train_images_normalised, (1, 2, 0))\n",
    "# test_images_normalised = np.transpose(test_images_normalised, (1, 2, 0))\n",
    "\n",
    "# print(train_images_normalised.shape)\n",
    "# print(test_images_normalised.shape)\n",
    "\n",
    "\n",
    "# print(label_fine.shape)\n",
    "# print(test_label_fine.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "useful-double",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "20\n",
      "(50000, 32, 32)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "#check number of unique  labels in coarse dtaaset\n",
    "#number of labels\n",
    "\n",
    "output_num_fine = (np.unique(label_fine).shape[0])\n",
    "print(output_num_fine)\n",
    "\n",
    "output_num_coarse = (np.unique(label_coarse).shape[0])\n",
    "print(output_num_coarse)\n",
    "\n",
    "print(train_images_normalised.shape)\n",
    "print(label_coarse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "accepting-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_reshaped = train_images_normalised.reshape(50000, 32, 32, 1)\n",
    "test_images_reshaped = test_images_normalised.reshape(10000, 32, 32, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aware-crowd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 1)\n",
      "(10000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_images_reshaped.shape)\n",
    "print(test_images_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "intensive-northeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrn_split, xTst_split, yTrn_split, yTst_split = train_test_split(train_images_reshaped, label_coarse, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "waiting-ivory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 2.9964 - accuracy: 0.0499 - val_loss: 2.9963 - val_accuracy: 0.0478\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 2.9962 - accuracy: 0.0492 - val_loss: 2.9961 - val_accuracy: 0.0502\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 2.9958 - accuracy: 0.0470 - val_loss: 2.9961 - val_accuracy: 0.0463\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 2.9960 - accuracy: 0.0495 - val_loss: 2.9961 - val_accuracy: 0.0473\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 2.9958 - accuracy: 0.0490 - val_loss: 2.9961 - val_accuracy: 0.0473\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 2.9960 - accuracy: 0.0496 - val_loss: 2.9962 - val_accuracy: 0.0473\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 2.9960 - accuracy: 0.0480 - val_loss: 2.9963 - val_accuracy: 0.0473\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 2.9958 - accuracy: 0.0496 - val_loss: 2.9961 - val_accuracy: 0.0477\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 2.9959 - accuracy: 0.0492 - val_loss: 2.9962 - val_accuracy: 0.0463\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 2.9959 - accuracy: 0.0525 - val_loss: 2.9961 - val_accuracy: 0.0463\n"
     ]
    }
   ],
   "source": [
    "model.add(tf.keras.layers.Flatten()) #dense layers can oinly have 1d so flattern araray to one dimension\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(output_num_coarse, activation='softmax'))\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(xTrn_split, yTrn_split, epochs=10, \n",
    "                    validation_data=(xTst_split, yTst_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-robin",
   "metadata": {},
   "source": [
    "###  Predict labels for the testing set and check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-sitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-postage",
   "metadata": {},
   "source": [
    "### Plot models traing curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "conditional-senegal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 - 2s - loss: 2.9958 - accuracy: 0.0500\n",
      "0.05000000074505806\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(train_images_reshaped,  label_coarse, verbose=2)\n",
    "\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "improving-alias",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.9957950115203857, 0.05000000074505806]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_images_reshaped, test_label_coarse, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-wiring",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
