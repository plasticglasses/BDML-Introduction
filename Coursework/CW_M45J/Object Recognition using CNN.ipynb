{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ordered-recorder",
   "metadata": {},
   "source": [
    "# CW1 - Object Recognition using CNN\n",
    "To apply machine learning alorithms to clasify the testing images into object categories. Then use a model to perform classification and report quantitative results.\n",
    "\n",
    "Due: Monday 19th April"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-karma",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-turning",
   "metadata": {},
   "source": [
    "The aim is to evaluate the use of CNN's in image recognition and the affect of adding multiple layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-negotiation",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "modified-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import load\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import skimage.feature\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-democracy",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "There are 100 different categories of objects\n",
    "each has 500 images for training and 100 images for testing.\n",
    "Split the data into train and test sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "recovered-longer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images Shape: (32, 32, 3, 50000)\n",
      "Train Labels Fine Shape: (50000,)\n",
      "Train Labels Coarse Shape: (50000,)\n"
     ]
    }
   ],
   "source": [
    "images = np.load('trnImage.npy')\n",
    "label_fine = np.load('trnLabel_fine.npy')\n",
    "label_coarse = np.load('trnLabel_coarse.npy')\n",
    "\n",
    "#image_index = 1 # pick a specific image\n",
    "#image = images[:, :, :, image_index]\n",
    "\n",
    "test_images = np.load('tstImage.npy')\n",
    "test_label_fine = np.load('tstLabel_fine.npy')\n",
    "test_label_coarse = np.load('tstLabel_coarse.npy')\n",
    "\n",
    "print(f'Images Shape: {images.shape}')\n",
    "print(f'Train Labels Fine Shape: {label_fine.shape}')\n",
    "print(f'Train Labels Coarse Shape: {label_coarse.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-wright",
   "metadata": {},
   "source": [
    "# Shuffle data to ensure not ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "martial-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = shuffle(images, random_state=0)\n",
    "label_fine, label_coarse = shuffle(label_fine, label_coarse, random_state=0) #make sure the samples are not ordered\n",
    "\n",
    "\n",
    "test_images = shuffle(test_images, random_state=0)\n",
    "test_label_fine, test_label_coarse = shuffle(test_label_fine, test_label_coarse, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-chain",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#images_reshaped = np.transpose(images, (3, 0, 1, 2))[:, :, :, -1]\n",
    "#images_reshaped = np.expand_dims(images_reshaped, axis=3)\n",
    "#\n",
    "#print(images_reshaped.shape)\n",
    "#print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_images_reshaped = np.transpose(test_images, (3, 0, 1, 2))[:, :, :, -1]\n",
    "#test_images_reshaped = np.expand_dims(test_images_reshaped, axis=3)\n",
    "\n",
    "#print(test_images_reshaped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-constraint",
   "metadata": {},
   "source": [
    "# Normalise the data, for each image do a hog, add how to array, train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "banner-psychiatry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_images_normalised = load('hog_array_train.npy')\n",
    "    print(train_images_normalised.shape)\n",
    "except FileNotFoundError: \n",
    "    \n",
    "    train_images_normalised = []\n",
    "    \n",
    "    for image_index in range(0, images.shape[3]):\n",
    "        print(image_index, images.shape[3])\n",
    "        image = images[:, :, :, image_index]\n",
    "\n",
    "        # Extract features from a single image\n",
    "        _, hog_image = skimage.feature.hog(image, pixels_per_cell=[2,2], cells_per_block=[3,3], visualize=True)\n",
    "        train_images_normalised.append(hog_image)\n",
    "        \n",
    "    train_images_output = np.array(train_images_normalised)\n",
    "    data = asarray(train_images_output)\n",
    "        # save to npy file\n",
    "    save('hog_array_train.npy', data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-tokyo",
   "metadata": {},
   "source": [
    "normalise test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "absolute-arrival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test_images_normalised = load('hog_array_test.npy')\n",
    "    print(test_images_normalised.shape)\n",
    "    \n",
    "except: \n",
    "    test_images_normalised = []\n",
    "    for image_index in range(0, test_images.shape[3]):\n",
    "        print(image_index, test_images.shape[3])\n",
    "        image = test_images[:, :, :, image_index]\n",
    "\n",
    "        # Extract features from a single image\n",
    "        _, hog_image = skimage.feature.hog(image, pixels_per_cell=[2,2], cells_per_block=[3,3], visualize=True)\n",
    "        test_images_normalised.append(hog_image)\n",
    "\n",
    "    train_images_output = np.array(test_images_normalised)\n",
    "    data = asarray(train_images_output)\n",
    "    save('hog_array_test.npy', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-opportunity",
   "metadata": {},
   "source": [
    "### Create and train a Tensorflow Convolutional Neural Network on the training set using Conv2D and pooling Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "interior-penetration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "=================================================================\n",
      "Total params: 320\n",
      "Trainable params: 320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "nuclear-wyoming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32)\n",
      "(10000, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(train_images_normalised.shape)\n",
    "print(test_images_normalised.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-guyana",
   "metadata": {},
   "source": [
    "Check the Data cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_images_normalised.shape)\n",
    "# print(test_images_normalised.shape)\n",
    "\n",
    "# #train_images_normalised = np.array(train_images_normalised).reshape(50000, -1, 32, 32)\n",
    "# #test_images_normalised = np.array(test_images_normalised).reshape(10000, -1, 32, 32)\n",
    "\n",
    "# #train_images_normalised = train_images_normalised[:, 0, :, :]\n",
    "# test_images_normalised = test_images_normalised[:, 0, :, :]\n",
    "\n",
    "# train_images_normalised = np.transpose(train_images_normalised, (1, 2, 0))\n",
    "# test_images_normalised = np.transpose(test_images_normalised, (1, 2, 0))\n",
    "\n",
    "# print(train_images_normalised.shape)\n",
    "# print(test_images_normalised.shape)\n",
    "\n",
    "\n",
    "# print(label_fine.shape)\n",
    "# print(test_label_fine.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "useful-double",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "20\n",
      "(50000, 32, 32)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "#check number of unique  labels in coarse dtaaset\n",
    "#number of labels\n",
    "\n",
    "output_num_fine = (np.unique(label_fine).shape[0])\n",
    "print(output_num_fine)\n",
    "\n",
    "output_num_coarse = (np.unique(label_coarse).shape[0])\n",
    "print(output_num_coarse)\n",
    "\n",
    "print(train_images_normalised.shape)\n",
    "print(label_coarse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "intensive-northeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrn_split, xTst_split, yTrn_split, yTst_split = train_test_split(train_images_normalised, label_coarse, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Flatten()) #dense layers can oinly have 1d so flattern araray to one dimension\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(output_num_coarse, activation='softmax'))\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(xTrn_split, yTrn_split, epochs=10, \n",
    "                    validation_data=(xTst_split, yTst_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-postage",
   "metadata": {},
   "source": [
    "### Plot models traing curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(images_normalised,  label_fine_normalised, verbose=2)\n",
    "\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_images_normalised, test_label_fine_normalised, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-wiring",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
