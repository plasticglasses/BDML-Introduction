{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ordered-recorder",
   "metadata": {},
   "source": [
    "# CW1 - Object Recognition using CNN\n",
    "To apply machine learning alorithms to clasify the testing images into object categories. Then use a model to perform classification and report quantitative results.\n",
    "\n",
    "Due: Monday 19th April"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-karma",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-turning",
   "metadata": {},
   "source": [
    "The aim is to evaluate the use of CNN's in image recognition and the affect of adding multiple layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-negotiation",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "modified-demographic",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-d6007d7415ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import load\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import skimage.feature\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-democracy",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "There are 100 different categories of objects\n",
    "each has 500 images for training and 100 images for testing.\n",
    "Split the data into train and test sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "recovered-longer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images Shape: (32, 32, 3, 50000)\n",
      "Train Labels Fine Shape: (50000,)\n",
      "Train Labels Coarse Shape: (50000,)\n"
     ]
    }
   ],
   "source": [
    "images = np.load('trnImage.npy')\n",
    "label_fine = np.load('trnLabel_fine.npy')\n",
    "label_coarse = np.load('trnLabel_coarse.npy')\n",
    "\n",
    "#image_index = 1 # pick a specific image\n",
    "#image = images[:, :, :, image_index]\n",
    "\n",
    "test_images = np.load('tstImage.npy')\n",
    "test_label_fine = np.load('tstLabel_fine.npy')\n",
    "test_label_coarse = np.load('tstLabel_coarse.npy')\n",
    "\n",
    "print(f'Images Shape: {images.shape}')\n",
    "print(f'Train Labels Fine Shape: {label_fine.shape}')\n",
    "print(f'Train Labels Coarse Shape: {label_coarse.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-wright",
   "metadata": {},
   "source": [
    "## Shuffle data to ensure not ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "martial-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = shuffle(images, random_state=0)\n",
    "label_fine, label_coarse = shuffle(label_fine, label_coarse, random_state=0) #make sure the samples are not ordered\n",
    "\n",
    "\n",
    "test_images = shuffle(test_images, random_state=0)\n",
    "test_label_fine, test_label_coarse = shuffle(test_label_fine, test_label_coarse, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-chain",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-constraint",
   "metadata": {},
   "source": [
    "## Normalise the data, for each image do a hog, add how to array, train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "banner-psychiatry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_images_normalised = load('hog_array_train.npy')\n",
    "    print(train_images_normalised.shape)\n",
    "except FileNotFoundError: \n",
    "    \n",
    "    train_images_normalised = []\n",
    "    \n",
    "    for image_index in range(0, images.shape[3]):\n",
    "        print(image_index, images.shape[3])\n",
    "        image = images[:, :, :, image_index]\n",
    "\n",
    "        # Extract features from a single image\n",
    "        _, hog_image = skimage.feature.hog(image, pixels_per_cell=[2,2], cells_per_block=[3,3], visualize=True)\n",
    "        train_images_normalised.append(hog_image)\n",
    "        \n",
    "    train_images_output = np.array(train_images_normalised)\n",
    "    data = asarray(train_images_output)\n",
    "        # save to npy file\n",
    "    save('hog_array_train.npy', data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "absolute-arrival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test_images_normalised = load('hog_array_test.npy')\n",
    "    print(test_images_normalised.shape)\n",
    "    \n",
    "except: \n",
    "    test_images_normalised = []\n",
    "    for image_index in range(0, test_images.shape[3]):\n",
    "        print(image_index, test_images.shape[3])\n",
    "        image = test_images[:, :, :, image_index]\n",
    "\n",
    "        # Extract features from a single image\n",
    "        _, hog_image = skimage.feature.hog(image, pixels_per_cell=[2,2], cells_per_block=[3,3], visualize=True)\n",
    "        test_images_normalised.append(hog_image)\n",
    "\n",
    "    train_images_output = np.array(test_images_normalised)\n",
    "    data = asarray(train_images_output)\n",
    "    save('hog_array_test.npy', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-guyana",
   "metadata": {},
   "source": [
    "## Check the Data cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "limiting-spotlight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (50000, 32, 32, 1)\n",
      "test shape (10000, 32, 32, 1)\n",
      "\n",
      "coarse shape (50000,)\n",
      "fine shape (50000,)\n",
      "\n",
      "Unique Fine labels 100\n",
      "unique Coarse Labels 20\n"
     ]
    }
   ],
   "source": [
    "train_images_reshaped = train_images_normalised.reshape(50000, 32, 32, 1)\n",
    "test_images_reshaped = test_images_normalised.reshape(10000, 32, 32, 1)\n",
    "\n",
    "print(\"train shape \" + str(train_images_reshaped.shape))\n",
    "print(\"test shape \" + str(test_images_reshaped.shape) + \"\\n\")\n",
    "\n",
    "print(\"coarse shape \" + str(label_coarse.shape))\n",
    "print(\"fine shape \" + str(label_fine.shape)+ \"\\n\")\n",
    "\n",
    "output_num_fine = (np.unique(label_fine).shape[0])\n",
    "print(\"Unique Fine labels \" + str(output_num_fine))\n",
    "\n",
    "output_num_coarse = (np.unique(label_coarse).shape[0])\n",
    "print(\"unique Coarse Labels \" + str(output_num_coarse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-skiing",
   "metadata": {},
   "source": [
    "## Split Training data into a train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intensive-northeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrn_split, xTst_split, yTrn_split, yTst_split = train_test_split(train_images_reshaped, label_coarse, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-opportunity",
   "metadata": {},
   "source": [
    "## Create a Tensorflow Convolutional Neural Network on the training set using Conv2D and pooling Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "interior-penetration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_20 (Conv2D)           (None, 30, 30, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 12, 12, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 10, 10, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 5, 32)          0         \n",
      "=================================================================\n",
      "Total params: 28,064\n",
      "Trainable params: 28,064\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='elu', input_shape=(32, 32, 1)))\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='elu', input_shape=(32, 32, 1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='elu', input_shape=(32, 32, 1)))\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='elu', input_shape=(32, 32, 1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-michael",
   "metadata": {},
   "source": [
    "## Train and fit the model\n",
    "\n",
    "softmax - sed as the last activation function of a neural network to normalize the output of a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "waiting-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Flatten()) #dense layers can oinly have 1d so flattern araray to one dimension\n",
    "model.add(tf.keras.layers.Dense(output_num_coarse*2, activation='selu'))\n",
    "model.add(tf.keras.layers.Dense(output_num_coarse*2, activation='selu'))\n",
    "model.add(tf.keras.layers.Dense(output_num_coarse*2, activation='selu'))\n",
    "model.add(tf.keras.layers.Dense(output_num_coarse, activation='softmax'))\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "continuing-handling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 3.0119 - accuracy: 0.0527 - val_loss: 3.0028 - val_accuracy: 0.0485\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 36s 29ms/step - loss: 3.0002 - accuracy: 0.0508 - val_loss: 2.9993 - val_accuracy: 0.0472\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9997 - accuracy: 0.0489 - val_loss: 3.0156 - val_accuracy: 0.0492\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9986 - accuracy: 0.0520 - val_loss: 2.9980 - val_accuracy: 0.0522\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9992 - accuracy: 0.0518 - val_loss: 2.9975 - val_accuracy: 0.0496\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9983 - accuracy: 0.0523 - val_loss: 3.0006 - val_accuracy: 0.0464\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9988 - accuracy: 0.0508 - val_loss: 3.0018 - val_accuracy: 0.0498\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9983 - accuracy: 0.0486 - val_loss: 2.9976 - val_accuracy: 0.0472\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9978 - accuracy: 0.0500 - val_loss: 2.9971 - val_accuracy: 0.0473\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9975 - accuracy: 0.0523 - val_loss: 3.0002 - val_accuracy: 0.0490\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 2.9976 - accuracy: 0.0518 - val_loss: 2.9990 - val_accuracy: 0.0524\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9976 - accuracy: 0.0515 - val_loss: 2.9977 - val_accuracy: 0.0513\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9974 - accuracy: 0.0532 - val_loss: 2.9990 - val_accuracy: 0.0466\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9972 - accuracy: 0.0529 - val_loss: 2.9985 - val_accuracy: 0.0471\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9964 - accuracy: 0.0544 - val_loss: 2.9988 - val_accuracy: 0.0535\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9959 - accuracy: 0.0510 - val_loss: 2.9991 - val_accuracy: 0.0467\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9944 - accuracy: 0.0585 - val_loss: 2.9994 - val_accuracy: 0.0503\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9948 - accuracy: 0.0552 - val_loss: 3.0011 - val_accuracy: 0.0509\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9931 - accuracy: 0.0593 - val_loss: 3.0003 - val_accuracy: 0.0482\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9922 - accuracy: 0.0588 - val_loss: 3.0033 - val_accuracy: 0.0466\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 2.9901 - accuracy: 0.0613 - val_loss: 3.0028 - val_accuracy: 0.0523\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 37s 30ms/step - loss: 2.9885 - accuracy: 0.0613 - val_loss: 3.0061 - val_accuracy: 0.0472\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9853 - accuracy: 0.0635 - val_loss: 3.0086 - val_accuracy: 0.0483\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9849 - accuracy: 0.0654 - val_loss: 3.0093 - val_accuracy: 0.0503\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9797 - accuracy: 0.0685 - val_loss: 3.0165 - val_accuracy: 0.0489\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9727 - accuracy: 0.0757 - val_loss: 3.0141 - val_accuracy: 0.0456\n",
      "Epoch 27/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 2.9733 - accuracy: 0.0742 - val_loss: 3.0262 - val_accuracy: 0.0485\n",
      "Epoch 28/100\n",
      "1111/1250 [=========================>....] - ETA: 3s - loss: 2.9661 - accuracy: 0.0773"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-ab810548759e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m history = model.fit(xTrn_split, yTrn_split, epochs=100, \n\u001b[1;32m----> 2\u001b[1;33m                     validation_data=(xTst_split, yTst_split))\n\u001b[0m",
      "\u001b[1;32mc:\\users\\lizks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lizks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lizks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lizks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lizks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lizks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\lizks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(xTrn_split, yTrn_split, epochs=100, \n",
    "                    validation_data=(xTst_split, yTst_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-street",
   "metadata": {},
   "source": [
    "##  Predict labels for the testing set and check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "integrated-invalid",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.7208243e-05 1.3156892e-04 8.4406354e-02 ... 3.7541618e-03\n",
      "  6.8277662e-04 3.7798347e-06]\n",
      " [2.9059731e-05 1.1924543e-05 2.9391385e-06 ... 9.7554401e-03\n",
      "  3.2838815e-04 2.8069595e-02]\n",
      " [3.4600250e-02 1.7566413e-03 2.7504620e-01 ... 1.3324437e-01\n",
      "  4.5037097e-03 1.9794952e-03]\n",
      " ...\n",
      " [6.2927839e-08 4.5048827e-04 1.7672627e-04 ... 2.4777933e-04\n",
      "  4.0938912e-06 3.4276911e-07]\n",
      " [2.8218193e-02 7.2773695e-02 2.4289082e-01 ... 2.6041090e-01\n",
      "  1.3819711e-02 1.8909350e-02]\n",
      " [3.2735559e-10 9.5278481e-14 1.7691275e-09 ... 6.7903623e-14\n",
      "  7.1002150e-06 2.7496187e-08]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(test_images_reshaped))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-postage",
   "metadata": {},
   "source": [
    "## Plot models traing curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "conditional-senegal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 4.4725 - accuracy: 0.0471\n",
      "0.0471000000834465\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images_reshaped,  test_label_coarse, verbose=2)\n",
    "\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "improving-alias",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.472523212432861, 0.0471000000834465]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_images_reshaped, test_label_coarse, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-wiring",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
