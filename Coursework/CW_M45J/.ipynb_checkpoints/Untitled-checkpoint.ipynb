{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cutout(inputs,patch_size=18):\n",
    "    \n",
    "    '''T. Devries and G. W. Taylor. Improved regularization of convolutional neural networks with cutout. \n",
    "    http://arxiv.org/abs/1708.04552.'''\n",
    "    \n",
    "    #***************************************************************************************************\n",
    "    #Definition of cutout to use random pixel values for CIFAR10 and CIFAR100 as in https://arxiv.org/abs/1802.08530\n",
    "    #M. D. McDonnell, Training wide residual networks for deployment using a single bit for each weight\n",
    "    #ICLR, 2018\n",
    "    #***************************************************************************************************\n",
    "\n",
    "    img_h, img_w, img_c = inputs.shape\n",
    "    Loc1 = np.random.randint(2-patch_size,img_h+1)\n",
    "    Loc2 = np.random.randint(2-patch_size,img_w+1)\n",
    "    top = np.maximum(0,Loc1-1)\n",
    "    bottom = np.minimum(img_h-1,Loc1+patch_size-1-1)\n",
    "    left = np.maximum(0,Loc2-1)\n",
    "    right = np.minimum(img_w-1,Loc2+patch_size-1-1)\n",
    "           \n",
    "    if np.amax(inputs) <= 1.0:\n",
    "        inputs = inputs*255.0\n",
    "    inputs[top:bottom+1, left:right+1, :] = np.random.randint(0, 256, (bottom-top+1, right-left+1, img_c))\n",
    "\n",
    "    #now fill padding with random integers. I set shifting to fill with constant = -1\n",
    "    Mask=inputs<0\n",
    "    c = np.count_nonzero(Mask)\n",
    "    inputs[Mask] = np.random.randint(0,256,c)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def GetDataGen(UseCutout): \n",
    "    \n",
    "    #***************************************************************************************************\n",
    "    #Definition of data augmentation for CIFAR10 and CIFAR100 as in https://arxiv.org/abs/1802.08530\n",
    "    #M. D. McDonnell, Training wide residual networks for deployment using a single bit for each weight\n",
    "    #ICLR, 2018\n",
    "    #***************************************************************************************************\n",
    "    \n",
    "    #in 2018, keras docs for when the preprocessing function is applied are wrong. Its actually \n",
    "    #applied last. This is good. It means my random valued padding and cutout works correctly\n",
    "\n",
    "    if UseCutout:\n",
    "        datagen = ImageDataGenerator(preprocessing_function=cutout,\n",
    "                                     width_shift_range=5,#using 4/32  excludes 0 shift. But 5 shifts by -4,-3,-2,-1,0,1,2,3,4\n",
    "                                     height_shift_range=5,\n",
    "                                     horizontal_flip=True,\n",
    "                                     fill_mode='constant',cval=-1) #note my cutout function changes cval=-1 to random \n",
    "                                                                 #ints so cutout is needed for this bit\n",
    "    else:\n",
    "        datagen = ImageDataGenerator(width_shift_range=5,\n",
    "                                     height_shift_range=5,\n",
    "                                     horizontal_flip=True,\n",
    "                                     fill_mode='constant',cval=0)\n",
    "        \n",
    "    return datagen\n",
    "\n",
    "\n",
    "#for implementing warm restarts in learning rate\n",
    "class LR_WarmRestart(tensorflow.keras.callbacks.Callback):\n",
    "    \n",
    "    '''I. Loshchilov and F. Hutter. SGDR: stochastic gradient descent with restarts.\n",
    "    http://arxiv.org/abs/1608.03983.'''\n",
    "    \n",
    "    def __init__(self,nbatch,initial_lr,min_lr,epochs_restart,Tmult=0.0):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.nbatch = nbatch\n",
    "        self.currentEP=0.0\n",
    "        self.startEP=1.0\n",
    "        self.ThisBatch = 0.0\n",
    "        self.lr_used=[]\n",
    "        self.Tmult=Tmult\n",
    "        self.epochs_restart=epochs_restart\n",
    "        self.Init=False\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.currentEP = self.currentEP+1.0\n",
    "        self.ThisBatch = 0.0\n",
    "        if self.Init==False:\n",
    "            K.set_value(self.model.optimizer.lr,self.initial_lr)\n",
    "            self.Init=True\n",
    "        if np.isin(self.currentEP,self.epochs_restart):\n",
    "            self.startEP=self.currentEP\n",
    "            self.Tmult=self.currentEP+1.0\n",
    "            K.set_value(self.model.optimizer.lr,self.initial_lr)\n",
    "        print ('\\n Start of Epoch Learning Rate = {:.6f}'.format(K.get_value(self.model.optimizer.lr)))\n",
    "\n",
    "    def on_epoch_end(self, epochs, logs={}):\n",
    "        print ('\\n End of Epoch Learning Rate = {:.6f}'.format(self.lr_used[-1]))\n",
    "\n",
    "        \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        \n",
    "        pts = self.currentEP - self.startEP + self.ThisBatch/(self.nbatch-1.0)\n",
    "        decay = 1.0+np.cos(pts/self.Tmult*np.pi)\n",
    "        newlr = self.min_lr+0.5*(self.initial_lr-self.min_lr)*decay\n",
    "        K.set_value(self.model.optimizer.lr,newlr)\n",
    "        \n",
    "        #keep track of what we  use in this batch\n",
    "        self.lr_used.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.ThisBatch = self.ThisBatch + 1.0\n",
    " \n",
    "       \n",
    "def plot_history(history):\n",
    "    \n",
    "    epochs = len(history['loss'])\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(range(1, epochs+1), history['loss'], label='loss')\n",
    "    plt.plot(range(1, epochs+1), history['val_loss'],'g',label='val_loss')\n",
    "    min_pos = np.argmin(history['val_loss'])\n",
    "    min_value = history['val_loss'][min_pos]\n",
    "    plt.scatter(min_pos+1, min_value, color='green', marker='*', label='min = ({}, {:.4f})'.format(min_pos+1, min_value))\n",
    "    plt.ylabel('loss'); plt.xlabel('epoch'); plt.legend(); plt.grid()\n",
    "    plt.subplot(122)\n",
    "    plt.plot(range(1, epochs+1), history['acc'], label='acc')\n",
    "    plt.plot(range(1, epochs+1), history['val_acc'],'g',label='val_acc')\n",
    "    max_pos = np.argmax(history['val_acc'])\n",
    "    max_value = history['val_acc'][max_pos]\n",
    "    plt.scatter(max_pos+1, max_value, color='green', marker='*', label='max = ({}, {:.4f})'.format(max_pos+1, max_value))\n",
    "    plt.ylabel('accuracy'); plt.xlabel('epoch'); plt.legend(); plt.grid()\n",
    "    plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
