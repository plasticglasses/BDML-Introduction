{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ordered-recorder",
   "metadata": {},
   "source": [
    "# CW1 - Object Recognition using CNN\n",
    "To apply machine learning alorithms to clasify the testing images into object categories. Then use a model to perform classification and report quantitative results.\n",
    "\n",
    "Due: Monday 19th April"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-karma",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-turning",
   "metadata": {},
   "source": [
    "The aim is to evaluate the use of CNN's in image recognition and the affect of adding multiple layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-negotiation",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "modified-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import load\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import skimage.feature\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-democracy",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "There are 100 different categories of objects\n",
    "each has 500 images for training and 100 images for testing.\n",
    "Split the data into train and test sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "recovered-longer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images Shape: (32, 32, 3, 50000)\n",
      "Train Labels Fine Shape: (50000,)\n",
      "Train Labels Coarse Shape: (50000,)\n"
     ]
    }
   ],
   "source": [
    "images = np.load('trnImage.npy')\n",
    "label_fine = np.load('trnLabel_fine.npy')\n",
    "label_coarse = np.load('trnLabel_coarse.npy')\n",
    "\n",
    "#image_index = 1 # pick a specific image\n",
    "#image = images[:, :, :, image_index]\n",
    "\n",
    "test_images = np.load('tstImage.npy')\n",
    "test_label_fine = np.load('tstLabel_fine.npy')\n",
    "test_label_coarse = np.load('tstLabel_coarse.npy')\n",
    "\n",
    "print(f'Images Shape: {images.shape}')\n",
    "print(f'Train Labels Fine Shape: {label_fine.shape}')\n",
    "print(f'Train Labels Coarse Shape: {label_coarse.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-wright",
   "metadata": {},
   "source": [
    "## Shuffle data to ensure not ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "martial-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = shuffle(images, random_state=0)\n",
    "label_fine, label_coarse = shuffle(label_fine, label_coarse, random_state=0) #make sure the samples are not ordered\n",
    "\n",
    "\n",
    "test_images = shuffle(test_images, random_state=0)\n",
    "test_label_fine, test_label_coarse = shuffle(test_label_fine, test_label_coarse, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-chain",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-constraint",
   "metadata": {},
   "source": [
    "## Normalise the data, for each image do a hog, add how to array, train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "banner-psychiatry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_images_normalised = load('hog_array_train.npy')\n",
    "    print(train_images_normalised.shape)\n",
    "except FileNotFoundError: \n",
    "    \n",
    "    train_images_normalised = []\n",
    "    \n",
    "    for image_index in range(0, images.shape[3]):\n",
    "        print(image_index, images.shape[3])\n",
    "        image = images[:, :, :, image_index]\n",
    "\n",
    "        # Extract features from a single image\n",
    "        _, hog_image = skimage.feature.hog(image, pixels_per_cell=[2,2], cells_per_block=[3,3], visualize=True)\n",
    "        train_images_normalised.append(hog_image)\n",
    "        \n",
    "    train_images_output = np.array(train_images_normalised)\n",
    "    data = asarray(train_images_output)\n",
    "        # save to npy file\n",
    "    save('hog_array_train.npy', data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "absolute-arrival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test_images_normalised = load('hog_array_test.npy')\n",
    "    print(test_images_normalised.shape)\n",
    "    \n",
    "except: \n",
    "    test_images_normalised = []\n",
    "    for image_index in range(0, test_images.shape[3]):\n",
    "        print(image_index, test_images.shape[3])\n",
    "        image = test_images[:, :, :, image_index]\n",
    "\n",
    "        # Extract features from a single image\n",
    "        _, hog_image = skimage.feature.hog(image, pixels_per_cell=[2,2], cells_per_block=[3,3], visualize=True)\n",
    "        test_images_normalised.append(hog_image)\n",
    "\n",
    "    train_images_output = np.array(test_images_normalised)\n",
    "    data = asarray(train_images_output)\n",
    "    save('hog_array_test.npy', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-guyana",
   "metadata": {},
   "source": [
    "## Check the Data cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "addressed-plain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (50000, 32, 32, 1)\n",
      "test shape (10000, 32, 32, 1)\n",
      "\n",
      "coarse shape (50000,)\n",
      "fine shape (50000,)\n",
      "\n",
      "Unique Fine labels 100\n",
      "unique Coarse Labels 20\n"
     ]
    }
   ],
   "source": [
    "train_images_reshaped = train_images_normalised.reshape(50000, 32, 32, 1)\n",
    "test_images_reshaped = test_images_normalised.reshape(10000, 32, 32, 1)\n",
    "\n",
    "print(\"train shape \" + str(train_images_reshaped.shape))\n",
    "print(\"test shape \" + str(test_images_reshaped.shape) + \"\\n\")\n",
    "\n",
    "print(\"coarse shape \" + str(label_coarse.shape))\n",
    "print(\"fine shape \" + str(label_fine.shape)+ \"\\n\")\n",
    "\n",
    "output_num_fine = (np.unique(label_fine).shape[0])\n",
    "print(\"Unique Fine labels \" + str(output_num_fine))\n",
    "\n",
    "output_num_coarse = (np.unique(label_coarse).shape[0])\n",
    "print(\"unique Coarse Labels \" + str(output_num_coarse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-arlington",
   "metadata": {},
   "source": [
    "## Split Training data into a train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intensive-northeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrn_split, xTst_split, yTrn_split, yTst_split = train_test_split(train_images_reshaped, label_coarse, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-opportunity",
   "metadata": {},
   "source": [
    "## Create a Tensorflow Convolutional Neural Network on the training set using Conv2D and pooling Layers\n",
    "AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "interior-penetration",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_50 (Conv2D)           (None, 30, 30, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 15, 15, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 13, 13, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 6, 6, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 4, 4, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 2, 2, 32)          128       \n",
      "=================================================================\n",
      "Total params: 19,200\n",
      "Trainable params: 19,008\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)))\n",
    "model.add(keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)))\n",
    "model.add(keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)))\n",
    "\n",
    "          \n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(4096, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(4096, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-jacob",
   "metadata": {},
   "source": [
    "## Train and fit the model\n",
    "\n",
    "softmax - sed as the last activation function of a neural network to normalize the output of a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "waiting-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "neural-cargo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 3.2968 - accuracy: 0.0512 - val_loss: 3.0045 - val_accuracy: 0.0496\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 3.0319 - accuracy: 0.0508 - val_loss: 2.9997 - val_accuracy: 0.0517\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 3.0056 - accuracy: 0.0503 - val_loss: 2.9989 - val_accuracy: 0.0501\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9994 - accuracy: 0.0544 - val_loss: 2.9980 - val_accuracy: 0.0476\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9960 - accuracy: 0.0531 - val_loss: 2.9978 - val_accuracy: 0.0478\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9951 - accuracy: 0.0536 - val_loss: 2.9970 - val_accuracy: 0.0499\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9940 - accuracy: 0.0562 - val_loss: 2.9976 - val_accuracy: 0.0448\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9941 - accuracy: 0.0542 - val_loss: 2.9977 - val_accuracy: 0.0512\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9933 - accuracy: 0.0551 - val_loss: 2.9972 - val_accuracy: 0.0522\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9920 - accuracy: 0.0584 - val_loss: 2.9975 - val_accuracy: 0.0534\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9919 - accuracy: 0.0600 - val_loss: 2.9981 - val_accuracy: 0.0527\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9906 - accuracy: 0.0612 - val_loss: 2.9980 - val_accuracy: 0.0527\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9892 - accuracy: 0.0583 - val_loss: 2.9984 - val_accuracy: 0.0520\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9902 - accuracy: 0.0602 - val_loss: 2.9994 - val_accuracy: 0.0492\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9883 - accuracy: 0.0617 - val_loss: 3.0000 - val_accuracy: 0.0521\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9884 - accuracy: 0.0599 - val_loss: 3.0012 - val_accuracy: 0.0549\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9868 - accuracy: 0.0636 - val_loss: 3.0003 - val_accuracy: 0.0527\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9863 - accuracy: 0.0602 - val_loss: 3.0002 - val_accuracy: 0.0531\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9846 - accuracy: 0.0658 - val_loss: 2.9996 - val_accuracy: 0.0551\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9838 - accuracy: 0.0639 - val_loss: 2.9997 - val_accuracy: 0.0554\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9846 - accuracy: 0.0639 - val_loss: 3.0006 - val_accuracy: 0.0552\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9833 - accuracy: 0.0682 - val_loss: 3.0017 - val_accuracy: 0.0510\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9816 - accuracy: 0.0639 - val_loss: 3.0010 - val_accuracy: 0.0527\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9819 - accuracy: 0.0706 - val_loss: 3.0028 - val_accuracy: 0.0518\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9806 - accuracy: 0.0694 - val_loss: 3.0045 - val_accuracy: 0.0507\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9794 - accuracy: 0.0692 - val_loss: 3.0011 - val_accuracy: 0.0535\n",
      "Epoch 27/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9795 - accuracy: 0.0694 - val_loss: 3.0044 - val_accuracy: 0.0541\n",
      "Epoch 28/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9794 - accuracy: 0.0696 - val_loss: 3.0030 - val_accuracy: 0.0566\n",
      "Epoch 29/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9751 - accuracy: 0.0695 - val_loss: 3.0030 - val_accuracy: 0.0536\n",
      "Epoch 30/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9772 - accuracy: 0.0704 - val_loss: 3.0065 - val_accuracy: 0.0528\n",
      "Epoch 31/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9756 - accuracy: 0.0692 - val_loss: 3.0052 - val_accuracy: 0.0512\n",
      "Epoch 32/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9733 - accuracy: 0.0751 - val_loss: 3.0024 - val_accuracy: 0.0523\n",
      "Epoch 33/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9746 - accuracy: 0.0701 - val_loss: 3.0049 - val_accuracy: 0.0516\n",
      "Epoch 34/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9737 - accuracy: 0.0726 - val_loss: 3.0046 - val_accuracy: 0.0515\n",
      "Epoch 35/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9728 - accuracy: 0.0745 - val_loss: 3.0057 - val_accuracy: 0.0527\n",
      "Epoch 36/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9728 - accuracy: 0.0727 - val_loss: 3.0061 - val_accuracy: 0.0515\n",
      "Epoch 37/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9705 - accuracy: 0.0775 - val_loss: 3.0034 - val_accuracy: 0.0520\n",
      "Epoch 38/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9706 - accuracy: 0.0743 - val_loss: 3.0057 - val_accuracy: 0.0514\n",
      "Epoch 39/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9679 - accuracy: 0.0774 - val_loss: 3.0068 - val_accuracy: 0.0516\n",
      "Epoch 40/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9675 - accuracy: 0.0770 - val_loss: 3.0051 - val_accuracy: 0.0506\n",
      "Epoch 41/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9679 - accuracy: 0.0729 - val_loss: 3.0053 - val_accuracy: 0.0538\n",
      "Epoch 42/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9664 - accuracy: 0.0799 - val_loss: 3.0040 - val_accuracy: 0.0513\n",
      "Epoch 43/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9685 - accuracy: 0.0752 - val_loss: 3.0067 - val_accuracy: 0.0514\n",
      "Epoch 44/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9672 - accuracy: 0.0745 - val_loss: 3.0075 - val_accuracy: 0.0517\n",
      "Epoch 45/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9667 - accuracy: 0.0767 - val_loss: 3.0054 - val_accuracy: 0.0550\n",
      "Epoch 46/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9618 - accuracy: 0.0802 - val_loss: 3.0086 - val_accuracy: 0.0506\n",
      "Epoch 47/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9635 - accuracy: 0.0779 - val_loss: 3.0061 - val_accuracy: 0.0531\n",
      "Epoch 48/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9633 - accuracy: 0.0793 - val_loss: 3.0068 - val_accuracy: 0.0538\n",
      "Epoch 49/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9631 - accuracy: 0.0784 - val_loss: 3.0077 - val_accuracy: 0.0532\n",
      "Epoch 50/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9621 - accuracy: 0.0760 - val_loss: 3.0109 - val_accuracy: 0.0523\n",
      "Epoch 51/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9641 - accuracy: 0.0787 - val_loss: 3.0092 - val_accuracy: 0.0485\n",
      "Epoch 52/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9646 - accuracy: 0.0773 - val_loss: 3.0122 - val_accuracy: 0.0519\n",
      "Epoch 53/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9612 - accuracy: 0.0806 - val_loss: 3.0088 - val_accuracy: 0.0511\n",
      "Epoch 54/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9581 - accuracy: 0.0821 - val_loss: 3.0081 - val_accuracy: 0.0515\n",
      "Epoch 55/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9620 - accuracy: 0.0805 - val_loss: 3.0103 - val_accuracy: 0.0524\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9615 - accuracy: 0.0800 - val_loss: 3.0068 - val_accuracy: 0.0498\n",
      "Epoch 57/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9564 - accuracy: 0.0833 - val_loss: 3.0086 - val_accuracy: 0.0504\n",
      "Epoch 58/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9606 - accuracy: 0.0823 - val_loss: 3.0097 - val_accuracy: 0.0540\n",
      "Epoch 59/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9561 - accuracy: 0.0842 - val_loss: 3.0099 - val_accuracy: 0.0532\n",
      "Epoch 60/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9553 - accuracy: 0.0831 - val_loss: 3.0130 - val_accuracy: 0.0469\n",
      "Epoch 61/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9569 - accuracy: 0.0855 - val_loss: 3.0119 - val_accuracy: 0.0530\n",
      "Epoch 62/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9591 - accuracy: 0.0823 - val_loss: 3.0073 - val_accuracy: 0.0513\n",
      "Epoch 63/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9569 - accuracy: 0.0840 - val_loss: 3.0108 - val_accuracy: 0.0518\n",
      "Epoch 64/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9560 - accuracy: 0.0846 - val_loss: 3.0118 - val_accuracy: 0.0524\n",
      "Epoch 65/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9551 - accuracy: 0.0828 - val_loss: 3.0142 - val_accuracy: 0.0508\n",
      "Epoch 66/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9523 - accuracy: 0.0841 - val_loss: 3.0110 - val_accuracy: 0.0512\n",
      "Epoch 67/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9547 - accuracy: 0.0857 - val_loss: 3.0143 - val_accuracy: 0.0522\n",
      "Epoch 68/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9541 - accuracy: 0.0834 - val_loss: 3.0117 - val_accuracy: 0.0488\n",
      "Epoch 69/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9518 - accuracy: 0.0815 - val_loss: 3.0148 - val_accuracy: 0.0496\n",
      "Epoch 70/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9531 - accuracy: 0.0815 - val_loss: 3.0137 - val_accuracy: 0.0506\n",
      "Epoch 71/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9507 - accuracy: 0.0844 - val_loss: 3.0138 - val_accuracy: 0.0482\n",
      "Epoch 72/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9520 - accuracy: 0.0864 - val_loss: 3.0110 - val_accuracy: 0.0479\n",
      "Epoch 73/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9489 - accuracy: 0.0869 - val_loss: 3.0139 - val_accuracy: 0.0496\n",
      "Epoch 74/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9553 - accuracy: 0.0821 - val_loss: 3.0116 - val_accuracy: 0.0516\n",
      "Epoch 75/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9497 - accuracy: 0.0896 - val_loss: 3.0148 - val_accuracy: 0.0507\n",
      "Epoch 76/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9490 - accuracy: 0.0887 - val_loss: 3.0141 - val_accuracy: 0.0492\n",
      "Epoch 77/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9500 - accuracy: 0.0867 - val_loss: 3.0145 - val_accuracy: 0.0495\n",
      "Epoch 78/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9515 - accuracy: 0.0862 - val_loss: 3.0106 - val_accuracy: 0.0536\n",
      "Epoch 79/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9482 - accuracy: 0.0860 - val_loss: 3.0134 - val_accuracy: 0.0507\n",
      "Epoch 80/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9509 - accuracy: 0.0820 - val_loss: 3.0125 - val_accuracy: 0.0486\n",
      "Epoch 81/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9479 - accuracy: 0.0852 - val_loss: 3.0148 - val_accuracy: 0.0472\n",
      "Epoch 82/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9487 - accuracy: 0.0858 - val_loss: 3.0147 - val_accuracy: 0.0496\n",
      "Epoch 83/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9531 - accuracy: 0.0867 - val_loss: 3.0144 - val_accuracy: 0.0495\n",
      "Epoch 84/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9485 - accuracy: 0.0846 - val_loss: 3.0141 - val_accuracy: 0.0490\n",
      "Epoch 85/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9471 - accuracy: 0.0870 - val_loss: 3.0140 - val_accuracy: 0.0516\n",
      "Epoch 86/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9485 - accuracy: 0.0878 - val_loss: 3.0132 - val_accuracy: 0.0482\n",
      "Epoch 87/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9464 - accuracy: 0.0882 - val_loss: 3.0139 - val_accuracy: 0.0528\n",
      "Epoch 88/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9492 - accuracy: 0.0840 - val_loss: 3.0168 - val_accuracy: 0.0486\n",
      "Epoch 89/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9471 - accuracy: 0.0844 - val_loss: 3.0141 - val_accuracy: 0.0510\n",
      "Epoch 90/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9492 - accuracy: 0.0847 - val_loss: 3.0152 - val_accuracy: 0.0510\n",
      "Epoch 91/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9455 - accuracy: 0.0884 - val_loss: 3.0140 - val_accuracy: 0.0524\n",
      "Epoch 92/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9469 - accuracy: 0.0868 - val_loss: 3.0113 - val_accuracy: 0.0510\n",
      "Epoch 93/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9452 - accuracy: 0.0868 - val_loss: 3.0159 - val_accuracy: 0.0506\n",
      "Epoch 94/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9456 - accuracy: 0.0885 - val_loss: 3.0157 - val_accuracy: 0.0522\n",
      "Epoch 95/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9455 - accuracy: 0.0889 - val_loss: 3.0184 - val_accuracy: 0.0493\n",
      "Epoch 96/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9441 - accuracy: 0.0869 - val_loss: 3.0182 - val_accuracy: 0.0502\n",
      "Epoch 97/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9430 - accuracy: 0.0892 - val_loss: 3.0207 - val_accuracy: 0.0508\n",
      "Epoch 98/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9416 - accuracy: 0.0875 - val_loss: 3.0130 - val_accuracy: 0.0505\n",
      "Epoch 99/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9441 - accuracy: 0.0871 - val_loss: 3.0187 - val_accuracy: 0.0514\n",
      "Epoch 100/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.9419 - accuracy: 0.0924 - val_loss: 3.0157 - val_accuracy: 0.0510\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(xTrn_split, yTrn_split, epochs=100, \n",
    "                    validation_data=(xTst_split, yTst_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-assembly",
   "metadata": {},
   "source": [
    "##  Predict labels for the testing set and check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "handy-steal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.7208243e-05 1.3156892e-04 8.4406354e-02 ... 3.7541618e-03\n",
      "  6.8277662e-04 3.7798347e-06]\n",
      " [2.9059731e-05 1.1924543e-05 2.9391385e-06 ... 9.7554401e-03\n",
      "  3.2838815e-04 2.8069595e-02]\n",
      " [3.4600250e-02 1.7566413e-03 2.7504620e-01 ... 1.3324437e-01\n",
      "  4.5037097e-03 1.9794952e-03]\n",
      " ...\n",
      " [6.2927839e-08 4.5048827e-04 1.7672627e-04 ... 2.4777933e-04\n",
      "  4.0938912e-06 3.4276911e-07]\n",
      " [2.8218193e-02 7.2773695e-02 2.4289082e-01 ... 2.6041090e-01\n",
      "  1.3819711e-02 1.8909350e-02]\n",
      " [3.2735559e-10 9.5278481e-14 1.7691275e-09 ... 6.7903623e-14\n",
      "  7.1002150e-06 2.7496187e-08]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(test_images_reshaped))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-postage",
   "metadata": {},
   "source": [
    "## Plot models traing curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "conditional-senegal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 4.4725 - accuracy: 0.0471\n",
      "0.0471000000834465\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images_reshaped,  test_label_coarse, verbose=2)\n",
    "\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "improving-alias",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.472523212432861, 0.0471000000834465]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_images_reshaped, test_label_coarse, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-wiring",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
