{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ordered-recorder",
   "metadata": {},
   "source": [
    "# CW1 - Object Recognition using CNN\n",
    "To apply machine learning alorithms to clasify the testing images into object categories. Then use a model to perform classification and report quantitative results.\n",
    "\n",
    "Due: Monday 19th April"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-karma",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-turning",
   "metadata": {},
   "source": [
    "The aim is to evaluate the use of CNN's in image recognition and the affect of adding multiple layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-negotiation",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "modified-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.feature\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-democracy",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "There are 100 different categories of objects\n",
    "each has 500 images for training and 100 images for testing.\n",
    "Split the data into train and test sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "recovered-longer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images Shape: (32, 32, 3, 50000)\n",
      "Train Labels Fine Shape: (50000,)\n",
      "Train Labels Coarse Shape: (50000,)\n"
     ]
    }
   ],
   "source": [
    "images = np.load('trnImage.npy')\n",
    "label_fine = np.load('trnLabel_fine.npy')\n",
    "label_coarse = np.load('trnLabel_coarse.npy')\n",
    "\n",
    "image_index = 1 # pick a specific image\n",
    "image = images[:, :, :, image_index]\n",
    "\n",
    "test_images = np.load('tstImage.npy')\n",
    "test_label_fine = np.load('tstLabel_fine.npy')\n",
    "test_label_coarse = np.load('tstLabel_coarse.npy')\n",
    "\n",
    "print(f'Images Shape: {images.shape}')\n",
    "print(f'Train Labels Fine Shape: {label_fine.shape}')\n",
    "print(f'Train Labels Coarse Shape: {label_coarse.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-bronze",
   "metadata": {},
   "source": [
    "# Visualise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "rational-syracuse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.scatter(images[:,0], images[:,1], color=\"r\", marker=\"o\")\n",
    "# #plt.scatter(test_images[:,0], test_images[:,1], color=\"b\", marker='v')\n",
    "\n",
    "# # Add title and axis names\n",
    "# plt.title('CIFAR-100 data using coarse')\n",
    "# plt.xlabel('height')\n",
    "# plt.ylabel('width')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-chain",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-threat",
   "metadata": {},
   "source": [
    "## Expland the data to include a channel dimension for CNN. As images are greyscale add one additional axis in the last dimension of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hybrid-greece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3, 50000)\n",
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(images.shape)\n",
    "images_reshaped = np.transpose(images, (3, 0, 1, 2))\n",
    "\n",
    "print(images_reshaped.shape)\n",
    "#images = np.expand_dims(images, axis=(4))\n",
    "#print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "forced-friend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(test_images.shape)\n",
    "test_images_reshaped = np.transpose(test_images, (3, 0, 1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-heritage",
   "metadata": {},
   "source": [
    "### Normalise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "painted-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_normalised= np.divide(images_reshaped, 255)\n",
    "test_images_normalised = np.divide(test_images_reshaped, 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-opportunity",
   "metadata": {},
   "source": [
    "### Create and train a Tensorflow Convolutional Neural Network on the training set using Conv2D and pooling Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "interior-penetration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "=================================================================\n",
      "Total params: 896\n",
      "Trainable params: 896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "waiting-ivory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Received a label value of 99 which is outside the valid range of [0, 10).  Label values: 0 23 81 46 90 45 3 50 59 13 1 88 62 79 65 73 34 58 3 99 36 85 10 19 33 37 64 90 29 96 73 74\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-21-57cb3b7dc529>:9) ]] [Op:__inference_train_function_2125]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-57cb3b7dc529>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m history = model.fit(images_reshaped, label_fine, epochs=10, \n\u001b[1;32m----> 9\u001b[1;33m                     validation_data=(test_images_reshaped, test_label_fine))\n\u001b[0m",
      "\u001b[1;32mc:\\users\\lizks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lizks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lizks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lizks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lizks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lizks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\lizks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  Received a label value of 99 which is outside the valid range of [0, 10).  Label values: 0 23 81 46 90 45 3 50 59 13 1 88 62 79 65 73 34 58 3 99 36 85 10 19 33 37 64 90 29 96 73 74\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-21-57cb3b7dc529>:9) ]] [Op:__inference_train_function_2125]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(10))\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(images_normalised, label_fine, epochs=10, \n",
    "                    validation_data=(test_images_normalised, test_label_fine))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-postage",
   "metadata": {},
   "source": [
    "### Plot models traing curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(images,  images_fine, verbose=2)\n",
    "\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "improving-alias",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 32\n  y sizes: 10000\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-d29b5356e755>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_label_fine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\lizks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1365\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1366\u001b[1;33m             steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1368\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lizks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lizks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[1;31m# If batch_size is not passed but steps is, calculate from the input data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lizks\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1527\u001b[0m           label, \", \".join(str(i.shape[0]) for i in nest.flatten(single_data)))\n\u001b[0;32m   1528\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1529\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 32\n  y sizes: 10000\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_images, test_label_fine, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-wiring",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
