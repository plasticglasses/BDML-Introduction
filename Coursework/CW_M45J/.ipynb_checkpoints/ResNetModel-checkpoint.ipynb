{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import InputSpec, Conv2D, BatchNormalization, Activation,ReLU,Flatten\n",
    "from tensorflow.keras.layers import AveragePooling2D, Input, GlobalAveragePooling2D, Lambda, concatenate\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "    \n",
    "#***************************************************************************************************\n",
    "#Definition of the binary 2D conv layer as in https://arxiv.org/abs/1802.08530\n",
    "#M. D. McDonnell, Training wide residual networks for deployment using a single bit for each weight\n",
    "#ICLR, 2018\n",
    "#\n",
    "#Adapated by M.D. McDonnell from https://github.com/DingKe/nn_playground/blob/master/binarynet/binary_layers.py\n",
    "#\n",
    "#See also: https://stackoverflow.com/questions/36456436/how-can-i-define-only-the-gradient-for-a-tensorflow-subgraph/36480182\n",
    "#\n",
    "#***************************************************************************************************\n",
    "class BinaryConv2D(Conv2D):\n",
    "\n",
    "    def __init__(self, filters, **kwargs):\n",
    "        super(BinaryConv2D, self).__init__(filters, **kwargs)\n",
    "        \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        channel_axis = -1\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "\n",
    "        input_dim = int(input_shape[channel_axis])\n",
    "        if input_dim is None:\n",
    "            raise ValueError('The channel dimension of the inputs should be defined. Found `None`.')\n",
    "\n",
    "        #***************************************************************************************************\n",
    "        #Binary layer multiplier as in https://arxiv.org/abs/1802.08530\n",
    "        #M. D. McDonnell, Training wide residual networks for deployment using a single bit for each weight\n",
    "        #ICLR, 2018\n",
    "        self.multiplier=np.sqrt(2.0/float(self.kernel_size[0])/float(self.kernel_size[1])/float(input_dim))\n",
    "        #***************************************************************************************************\n",
    "        \n",
    "        self.kernel = self.add_weight(shape=self.kernel_size + (input_dim, self.filters),\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='kernel',\n",
    "                                 regularizer=self.kernel_regularizer,\n",
    "                                 constraint=self.kernel_constraint)\n",
    "\n",
    "        # Set input spec.\n",
    "        self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        #***************************************************************************************************\n",
    "        #Binary layer as in https://arxiv.org/abs/1802.08530\n",
    "        #M. D. McDonnell, Training wide residual networks for deployment using a single bit for each weight\n",
    "        #ICLR, 2018\n",
    "        #\n",
    "        #This code sets the full precsion weights to binary for forward and bacjkward propagation\n",
    "        #but enables gradients to update the full precision weights that ar used only during training \n",
    "        #\n",
    "        binary_kernel = self.kernel + K.stop_gradient(K.sign(self.kernel) - self.kernel)\n",
    "        binary_kernel=binary_kernel+K.stop_gradient(binary_kernel*self.multiplier-binary_kernel)\n",
    "        #***************************************************************************************************\n",
    "        \n",
    "        outputs = K.conv2d( inputs,\n",
    "                            binary_kernel,\n",
    "                            strides=self.strides,\n",
    "                            padding=self.padding,\n",
    "                            data_format=self.data_format,\n",
    "                            dilation_rate=self.dilation_rate)\n",
    "\n",
    "\n",
    "        return outputs\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = {'multiplier': self.multiplier}\n",
    "        base_config = super(BinaryConv2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    \n",
    "#***************************************************************************************************\n",
    "#Definition of the  resnet variant in https://arxiv.org/abs/1802.08530\n",
    "#M. D. McDonnell, Training wide residual networks for deployment using a single bit for each weight\n",
    "#ICLR, 2018\n",
    "#***************************************************************************************************\n",
    "def resnet_layer(inputs,num_filters=16,kernel_size=3,strides=1,bn_moments_momentum=0.99,\n",
    "                 learn_bn = True,wd=1e-4,UseRelu=True,UseBN=True,UseBinaryWeights=False):\n",
    "    x = inputs\n",
    "    if UseBN:\n",
    "        #epsilon=1e-3 is keras default\n",
    "        x = BatchNormalization(epsilon=1e-5,momentum=bn_moments_momentum,center=learn_bn,scale=learn_bn)(x)\n",
    "    if UseRelu:\n",
    "        x = Activation('relu')(x)\n",
    "    if UseBinaryWeights:\n",
    "        x = BinaryConv2D(num_filters,\n",
    "                     kernel_size=kernel_size,\n",
    "                     strides=strides,\n",
    "                     padding='same',\n",
    "                     kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(wd),\n",
    "                     use_bias=False)(x)\n",
    "    else:\n",
    "        x = Conv2D(num_filters,\n",
    "                     kernel_size=kernel_size,\n",
    "                     strides=strides,\n",
    "                     padding='same',\n",
    "                     kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(wd),\n",
    "                     use_bias=False)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "#***************************************************************************************************\n",
    "#Definition of the resnet variant in https://arxiv.org/abs/1802.08530\n",
    "#M. D. McDonnell, Training wide residual networks for deployment using a single bit for each weight\n",
    "#ICLR, 2018\n",
    "#***************************************************************************************************\n",
    "def resnet(UseBinaryWeights,input_shape, depth, num_classes=10, width=1,wd=0.0):\n",
    "\n",
    "    # Start model definition.\n",
    "    base_filters = 16\n",
    "    num_filters = base_filters*width\n",
    "    \n",
    "    bn_moments_momentum = 0.99 #this is keras default\n",
    "    My_wd = wd\n",
    "    \n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    #input layers prior to first branching\n",
    "    inputs = Input(shape=input_shape)     \n",
    "    \n",
    "    \n",
    "    ResidualPath = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     bn_moments_momentum=bn_moments_momentum,\n",
    "                     learn_bn = True,\n",
    "                     wd=My_wd,\n",
    "                     UseRelu=False,\n",
    "                     UseBN=True,\n",
    "                     UseBinaryWeights=UseBinaryWeights)\n",
    "    \n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            ConvPath = resnet_layer(inputs=ResidualPath,\n",
    "                             num_filters=num_filters,\n",
    "                             kernel_size=3,\n",
    "                             strides=strides, #sometimes this is 2\n",
    "                             bn_moments_momentum=bn_moments_momentum,\n",
    "                             learn_bn = False,\n",
    "                             wd=My_wd,\n",
    "                             UseBN=True,\n",
    "                             UseBinaryWeights=UseBinaryWeights)\n",
    "            ConvPath = resnet_layer(inputs=ConvPath,\n",
    "                             num_filters=num_filters,\n",
    "                             kernel_size=3,\n",
    "                             strides=1,\n",
    "                             bn_moments_momentum=bn_moments_momentum,\n",
    "                             learn_bn = False,\n",
    "                             wd=My_wd,\n",
    "                             UseBN=True,\n",
    "                             UseBinaryWeights=UseBinaryWeights)\n",
    "            if stack > 0 and res_block == 0:  \n",
    "                # first layer but not first stack: this is where we have gone up in channels and down in feature map size\n",
    "                #so need to account for this in the residual path\n",
    "                #average pool and downsample the residual path\n",
    "                ResidualPath = AveragePooling2D(pool_size=(3, 3), strides=2, padding='same')(ResidualPath)\n",
    "                \n",
    "                #zero pad to increase channels\n",
    "                ResidualPath=concatenate([ResidualPath,Lambda(K.zeros_like)(ResidualPath)])\n",
    "\n",
    "            ResidualPath = tensorflow.keras.layers.add([ConvPath,ResidualPath])\n",
    "            \n",
    "        #when we are here, we double the number of filters    \n",
    "        num_filters *= 2\n",
    "\n",
    "    #output layers after last sum\n",
    "    OutputPath = resnet_layer(inputs=ResidualPath,\n",
    "                     num_filters=num_classes,\n",
    "                     strides = 1,\n",
    "                     kernel_size=1,\n",
    "                     bn_moments_momentum=bn_moments_momentum,\n",
    "                     learn_bn = False,\n",
    "                     wd=My_wd,\n",
    "                     UseBN=True,\n",
    "                     UseBinaryWeights=UseBinaryWeights)\n",
    "    OutputPath = BatchNormalization(epsilon=1e-5,momentum=bn_moments_momentum,center=False, scale=False)(OutputPath)\n",
    "    OutputPath = GlobalAveragePooling2D()(OutputPath)\n",
    "    OutputPath = Activation('softmax')(OutputPath)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=OutputPath)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet_layer_srelu(inputs,num_filters=16,kernel_size=3,strides=1,wd=1e-4,UseRelu=True,UseBinaryWeights=False):\n",
    "    x = inputs\n",
    "    if UseRelu:\n",
    "        #shifted relu: y = max(-1,x)\n",
    "        x=Lambda(lambda z: z + 1)(x)\n",
    "        x = ReLU()(x)\n",
    "        x=Lambda(lambda z: z - 1)(x)\n",
    "    if UseBinaryWeights:\n",
    "        x = BinaryConv2D(num_filters,\n",
    "                     kernel_size=kernel_size,\n",
    "                     strides=strides,\n",
    "                     padding='same',\n",
    "                     kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(wd),\n",
    "                     use_bias=False)(x)\n",
    "    else:\n",
    "        x = Conv2D(num_filters,\n",
    "                     kernel_size=kernel_size,\n",
    "                     strides=strides,\n",
    "                     padding='same',\n",
    "                     kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(wd),\n",
    "                     use_bias=False)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "#***************************************************************************************************\n",
    "\n",
    "#***************************************************************************************************\n",
    "def resnet_srelu(Temperature,UseBinaryWeights,input_shape, depth, num_classes=10, width=1,wd=0.0,use_softmax=False):\n",
    "\n",
    "    # Start model definition.\n",
    "    base_filters = 16\n",
    "    num_filters = base_filters*width\n",
    "    \n",
    "    My_wd = wd\n",
    "    \n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    #input layers prior to first branching\n",
    "    inputs = Input(shape=input_shape)     \n",
    "    \n",
    "    x = BatchNormalization(epsilon=1e-5,center=True,scale=True,renorm=False)(inputs)\n",
    "    ResidualPath = resnet_layer_srelu(inputs=x,\n",
    "                     num_filters=num_filters,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     wd=My_wd,\n",
    "                     UseRelu=False,\n",
    "                     UseBinaryWeights=UseBinaryWeights)\n",
    "    \n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            ConvPath = resnet_layer_srelu(inputs=ResidualPath,\n",
    "                             num_filters=num_filters,\n",
    "                             kernel_size=3,\n",
    "                             strides=strides, #sometimes this is 2\n",
    "                             wd=My_wd,\n",
    "                             UseBinaryWeights=UseBinaryWeights)\n",
    "            ConvPath = resnet_layer_srelu(inputs=ConvPath,\n",
    "                             num_filters=num_filters,\n",
    "                             kernel_size=3,\n",
    "                             strides=1,\n",
    "                             wd=My_wd,\n",
    "                             UseBinaryWeights=UseBinaryWeights)\n",
    "            if stack > 0 and res_block == 0:  \n",
    "                # first layer but not first stack: this is where we have gone up in channels and down in feature map size\n",
    "                #so need to account for this in the residual path\n",
    "                #average pool and downsample the residual path\n",
    "                ResidualPath = AveragePooling2D(pool_size=(3, 3), strides=2, padding='same')(ResidualPath)\n",
    "                \n",
    "                #zero pad to increase channels\n",
    "                ResidualPath=concatenate([ResidualPath,Lambda(K.zeros_like)(ResidualPath)])\n",
    "\n",
    "            ResidualPath = tensorflow.keras.layers.add([ConvPath,ResidualPath])\n",
    "            \n",
    "        #when we are here, we double the number of filters    \n",
    "        num_filters *= 2\n",
    "\n",
    "    #output layers after last sum\n",
    "    OutputPath = resnet_layer_srelu(inputs=ResidualPath,\n",
    "                     num_filters=num_classes,\n",
    "                     strides = 1,\n",
    "                     kernel_size=1,\n",
    "                     wd=My_wd,\n",
    "                     UseBinaryWeights=UseBinaryWeights)\n",
    "    OutputPath = Lambda(lambda x: x * (1.0/Temperature))(OutputPath)\n",
    "    OutputPath = GlobalAveragePooling2D()(OutputPath)\n",
    "    if use_softmax:\n",
    "        OutputPath = Activation('softmax')(OutputPath)\n",
    "    else:\n",
    "        pass#OutputPath= Lambda(K.squeeze,arguments={'axis':-1})(OutputPath)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=OutputPath)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
